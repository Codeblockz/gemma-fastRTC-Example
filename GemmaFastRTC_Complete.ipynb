{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Codeblockz/gemma-fastRTC-Example/blob/main/GemmaFastRTC_Complete.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvLoRe-fEjv8"
      },
      "source": [
        "# Gemma Voice Assistant with FastRTC\n",
        "\n",
        "This notebook implements a voice-based conversational AI assistant using:\n",
        "- Gemma 3 model from Hugging Face for text generation\n",
        "- FastRTC for real-time audio communication\n",
        "- Speech-to-text and text-to-speech capabilities\n",
        "- Conversation history tracking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnUpyQQHEjv_"
      },
      "source": [
        "## 1. Installation\n",
        "\n",
        "First, let's install the required packages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "o2MtKjCgEjv_",
        "outputId": "ac70bdfd-3f3d-4e83-f224-7d14075b959b"
      },
      "source": [
        "!pip install torch transformers fastrtc fastrtc[vad] fastrtc[stt] fastrtc[tts] python-dotenv gradio twilio\n",
        "!pip install git+https://github.com/huggingface/transformers@v4.49.0-Gemma-3\n",
        "!pip install accelerate"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Collecting fastrtc\n",
            "  Downloading fastrtc-0.0.16-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting gradio\n",
            "  Downloading gradio-5.22.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting twilio\n",
            "  Downloading twilio-9.5.0-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Collecting aiortc (from fastrtc)\n",
            "  Downloading aiortc-1.10.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (from fastrtc) (0.10.2.post1)\n",
            "Requirement already satisfied: numba>=0.60.0 in /usr/local/lib/python3.11/dist-packages (from fastrtc) (0.60.0)\n",
            "Collecting onnxruntime>=1.20.1 (from fastrtc[vad])\n",
            "  Downloading onnxruntime-1.21.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting fastrtc-moonshine-onnx (from fastrtc[stt])\n",
            "  Downloading fastrtc_moonshine_onnx-20241016-py3-none-any.whl.metadata (1.0 kB)\n",
            "Collecting kokoro-onnx (from fastrtc[tts])\n",
            "  Downloading kokoro_onnx-0.4.5-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.7.1)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.11-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.8.0 (from gradio)\n",
            "  Downloading gradio_client-1.8.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.15)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.6)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.11.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (14.2)\n",
            "Requirement already satisfied: PyJWT<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from twilio) (2.10.1)\n",
            "Requirement already satisfied: aiohttp>=3.8.4 in /usr/local/lib/python3.11/dist-packages (from twilio) (3.11.13)\n",
            "Collecting aiohttp-retry>=2.8.3 (from twilio)\n",
            "  Downloading aiohttp_retry-2.9.1-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.4->twilio) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.4->twilio) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.4->twilio) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.4->twilio) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.4->twilio) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.4->twilio) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.4->twilio) (1.18.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.60.0->fastrtc) (0.43.0)\n",
            "Collecting coloredlogs (from onnxruntime>=1.20.1->fastrtc[vad])\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.20.1->fastrtc[vad]) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.20.1->fastrtc[vad]) (4.25.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Collecting aioice<1.0.0,>=0.9.0 (from aiortc->fastrtc)\n",
            "  Downloading aioice-0.9.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting av<14.0.0,>=9.0.0 (from aiortc->fastrtc)\n",
            "  Downloading av-13.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: cffi>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from aiortc->fastrtc) (1.17.1)\n",
            "Requirement already satisfied: cryptography>=42.0.0 in /usr/local/lib/python3.11/dist-packages (from aiortc->fastrtc) (43.0.3)\n",
            "Requirement already satisfied: google-crc32c>=1.1 in /usr/local/lib/python3.11/dist-packages (from aiortc->fastrtc) (1.6.0)\n",
            "Collecting pyee>=9.0.0 (from aiortc->fastrtc)\n",
            "  Downloading pyee-13.0.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting pylibsrtp>=0.10.0 (from aiortc->fastrtc)\n",
            "  Downloading pylibsrtp-0.11.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: pyopenssl>=24.0.0 in /usr/local/lib/python3.11/dist-packages (from aiortc->fastrtc) (24.2.1)\n",
            "Collecting colorlog>=6.9.0 (from kokoro-onnx->fastrtc[tts])\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting espeakng-loader>=0.2.4 (from kokoro-onnx->fastrtc[tts])\n",
            "  Downloading espeakng_loader-0.2.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting phonemizer-fork==3.3.1 (from kokoro-onnx->fastrtc[tts])\n",
            "  Downloading phonemizer_fork-3.3.1-py3-none-any.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.3/48.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dlinfo (from phonemizer-fork==3.3.1->kokoro-onnx->fastrtc[tts])\n",
            "  Downloading dlinfo-2.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from phonemizer-fork==3.3.1->kokoro-onnx->fastrtc[tts]) (1.4.2)\n",
            "Collecting segments (from phonemizer-fork==3.3.1->kokoro-onnx->fastrtc[tts])\n",
            "  Downloading segments-2.3.0-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa->fastrtc) (3.0.1)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from librosa->fastrtc) (1.14.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from librosa->fastrtc) (1.6.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa->fastrtc) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa->fastrtc) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa->fastrtc) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa->fastrtc) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa->fastrtc) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->fastrtc) (1.1.0)\n",
            "Collecting dnspython>=2.0.0 (from aioice<1.0.0,>=0.9.0->aiortc->fastrtc)\n",
            "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting ifaddr>=0.2.0 (from aioice<1.0.0,>=0.9.0->aiortc->fastrtc)\n",
            "  Downloading ifaddr-0.2.0-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0.0->aiortc->fastrtc) (2.22)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa->fastrtc) (4.3.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.0->librosa->fastrtc) (3.6.0)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.20.1->fastrtc[vad])\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Collecting csvw>=1.5.6 (from segments->phonemizer-fork==3.3.1->kokoro-onnx->fastrtc[tts])\n",
            "  Downloading csvw-3.5.1-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Collecting isodate (from csvw>=1.5.6->segments->phonemizer-fork==3.3.1->kokoro-onnx->fastrtc[tts])\n",
            "  Downloading isodate-0.7.2-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting rfc3986<2 (from csvw>=1.5.6->segments->phonemizer-fork==3.3.1->kokoro-onnx->fastrtc[tts])\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: uritemplate>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from csvw>=1.5.6->segments->phonemizer-fork==3.3.1->kokoro-onnx->fastrtc[tts]) (4.1.1)\n",
            "Requirement already satisfied: babel in /usr/local/lib/python3.11/dist-packages (from csvw>=1.5.6->segments->phonemizer-fork==3.3.1->kokoro-onnx->fastrtc[tts]) (2.17.0)\n",
            "Collecting language-tags (from csvw>=1.5.6->segments->phonemizer-fork==3.3.1->kokoro-onnx->fastrtc[tts])\n",
            "  Downloading language_tags-1.2.0-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting rdflib (from csvw>=1.5.6->segments->phonemizer-fork==3.3.1->kokoro-onnx->fastrtc[tts])\n",
            "  Downloading rdflib-7.1.3-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting colorama (from csvw>=1.5.6->segments->phonemizer-fork==3.3.1->kokoro-onnx->fastrtc[tts])\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from csvw>=1.5.6->segments->phonemizer-fork==3.3.1->kokoro-onnx->fastrtc[tts]) (4.23.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->csvw>=1.5.6->segments->phonemizer-fork==3.3.1->kokoro-onnx->fastrtc[tts]) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema->csvw>=1.5.6->segments->phonemizer-fork==3.3.1->kokoro-onnx->fastrtc[tts]) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->csvw>=1.5.6->segments->phonemizer-fork==3.3.1->kokoro-onnx->fastrtc[tts]) (0.23.1)\n",
            "Requirement already satisfied: pyparsing<4,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from rdflib->csvw>=1.5.6->segments->phonemizer-fork==3.3.1->kokoro-onnx->fastrtc[tts]) (3.2.1)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m114.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m88.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m75.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastrtc-0.0.16-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m79.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading gradio-5.22.0-py3-none-any.whl (46.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.2/46.2 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.8.0-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading twilio-9.5.0-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading aiohttp_retry-2.9.1-py3-none-any.whl (10.0 kB)\n",
            "Downloading fastapi-0.115.11-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading onnxruntime-1.21.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.1-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiortc-1.10.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastrtc_moonshine_onnx-20241016-py3-none-any.whl (729 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m729.3/729.3 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading kokoro_onnx-0.4.5-py3-none-any.whl (17 kB)\n",
            "Downloading phonemizer_fork-3.3.1-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.5/82.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading aioice-0.9.0-py3-none-any.whl (24 kB)\n",
            "Downloading av-13.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.0/34.0 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Downloading espeakng_loader-0.2.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyee-13.0.0-py3-none-any.whl (15 kB)\n",
            "Downloading pylibsrtp-0.11.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ifaddr-0.2.0-py3-none-any.whl (12 kB)\n",
            "Downloading dlinfo-2.0.0-py3-none-any.whl (3.7 kB)\n",
            "Downloading segments-2.3.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading csvw-3.5.1-py2.py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.9/59.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading isodate-0.7.2-py3-none-any.whl (22 kB)\n",
            "Downloading language_tags-1.2.0-py3-none-any.whl (213 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.4/213.4 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rdflib-7.1.3-py3-none-any.whl (564 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.9/564.9 kB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rfc3986, pydub, language-tags, ifaddr, uvicorn, tomlkit, semantic-version, ruff, rdflib, python-multipart, python-dotenv, pyee, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, isodate, humanfriendly, groovy, ffmpy, espeakng-loader, dnspython, dlinfo, colorlog, colorama, av, aiofiles, starlette, pylibsrtp, nvidia-cusparse-cu12, nvidia-cudnn-cu12, coloredlogs, aioice, safehttpx, onnxruntime, nvidia-cusolver-cu12, gradio-client, fastapi, aiohttp-retry, twilio, gradio, fastrtc-moonshine-onnx, csvw, aiortc, segments, fastrtc, phonemizer-fork, kokoro-onnx\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed aiofiles-23.2.1 aiohttp-retry-2.9.1 aioice-0.9.0 aiortc-1.10.1 av-13.1.0 colorama-0.4.6 coloredlogs-15.0.1 colorlog-6.9.0 csvw-3.5.1 dlinfo-2.0.0 dnspython-2.7.0 espeakng-loader-0.2.4 fastapi-0.115.11 fastrtc-0.0.16 fastrtc-moonshine-onnx-20241016 ffmpy-0.5.0 gradio-5.22.0 gradio-client-1.8.0 groovy-0.1.2 humanfriendly-10.0 ifaddr-0.2.0 isodate-0.7.2 kokoro-onnx-0.4.5 language-tags-1.2.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 onnxruntime-1.21.0 phonemizer-fork-3.3.1 pydub-0.25.1 pyee-13.0.0 pylibsrtp-0.11.0 python-dotenv-1.0.1 python-multipart-0.0.20 rdflib-7.1.3 rfc3986-1.5.0 ruff-0.11.0 safehttpx-0.1.6 segments-2.3.0 semantic-version-2.10.0 starlette-0.46.1 tomlkit-0.13.2 twilio-9.5.0 uvicorn-0.34.0\n",
            "Collecting git+https://github.com/huggingface/transformers@v4.49.0-Gemma-3\n",
            "  Cloning https://github.com/huggingface/transformers (to revision v4.49.0-Gemma-3) to /tmp/pip-req-build-b3f82mh8\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-b3f82mh8\n",
            "  Running command git checkout -q 367bab469b0ef32017e2a0a0a5dbac5d36002f03\n",
            "  Resolved https://github.com/huggingface/transformers to commit 367bab469b0ef32017e2a0a0a5dbac5d36002f03\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.0.dev0) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.0.dev0) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.0.dev0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.0.dev0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.0.dev0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.0.dev0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.0.dev0) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.0.dev0) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.0.dev0) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.0.dev0) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers==4.50.0.dev0) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers==4.50.0.dev0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.50.0.dev0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.50.0.dev0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.50.0.dev0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.50.0.dev0) (2025.1.31)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.50.0.dev0-py3-none-any.whl size=10936434 sha256=0a14627a92b58727c57f4ce5c8045e272d8258dc90788e1ea7393443478d0174\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-v4lm2wip/wheels/53/15/d5/d63b866c641d8863f9cd29a4cc7a5efc38476c3aae8247c195\n",
            "Successfully built transformers\n",
            "Installing collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.48.3\n",
            "    Uninstalling transformers-4.48.3:\n",
            "      Successfully uninstalled transformers-4.48.3\n",
            "Successfully installed transformers-4.50.0.dev0\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.3.0)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.28.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2024.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBoOzw73EjwA"
      },
      "source": [
        "## 2. Imports and Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EUNG9O09EjwA"
      },
      "source": [
        "import os\n",
        "import base64\n",
        "import json\n",
        "import logging\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from typing import List, Dict, Any, Tuple, Optional, Union, Generator\n",
        "import asyncio\n",
        "import time\n",
        "\n",
        "import gradio as gr\n",
        "from fastapi import FastAPI\n",
        "from fastapi.responses import HTMLResponse, StreamingResponse\n",
        "from pydantic import BaseModel\n",
        "from fastrtc import (\n",
        "    AdditionalOutputs,\n",
        "    ReplyOnStopWords,\n",
        "    Stream,\n",
        "    get_stt_model,\n",
        "    get_tts_model,\n",
        "    get_twilio_turn_credentials,\n",
        ")\n",
        "from gradio.utils import get_space\n",
        "from dotenv import load_dotenv\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        ")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcLcyqBzEjwB"
      },
      "source": [
        "## 3. Environment Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "JOftdA23EjwB"
      },
      "source": [
        "# Set environment variables\n",
        "from google.colab import userdata\n",
        "os.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN')  # Replace with your actual token\n",
        "os.environ[\"USE_CUDA\"] = \"true\"  # Set to \"false\" if you don't have a GPU\n",
        "os.environ[\"USE_TWILIO\"] = \"true\"\n",
        "os.environ[\"TWILIO_ACCOUNT_SID\"] = userdata.get('TWILIO_ACCOUNT_SID')\n",
        "os.environ[\"TWILIO_AUTH_TOKEN\"] = userdata.get('TWILIO_AUTH_TOKEN')\n",
        "os.environ[\"SPEAKER_ID\"] = \"0\"\n",
        "os.environ[\"MODE\"] = \"UI\"  # Options: \"UI\" for Gradio, \"WEB\" for FastAPI web interface\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Constants\n",
        "SPEAKER_ID = int(os.environ.get(\"SPEAKER_ID\", \"0\"))\n",
        "MAX_CONTEXT_TURNS = 10\n",
        "MODEL_ID = \"google/gemma-3-1b-it\"\n",
        "DEVICE = \"cuda\" if os.environ.get(\"USE_CUDA\", \"true\").lower() == \"true\" else \"cpu\"\n",
        "MAX_LENGTH = 256\n",
        "\n",
        "# Get current directory\n",
        "curr_dir = Path.cwd()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVYFLWwfEjwB"
      },
      "source": [
        "## 4. Gemma Model Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "uE_WiSBrEjwC"
      },
      "source": [
        "class GemmaModel:\n",
        "    \"\"\"Class to handle Gemma 3 model interactions.\"\"\"\n",
        "\n",
        "    def __init__(self, model_id: str = MODEL_ID, device: str = DEVICE):\n",
        "        self.model_id = model_id\n",
        "        self.device = device\n",
        "        self.max_length = MAX_LENGTH\n",
        "\n",
        "        # Check if HF_TOKEN is set\n",
        "        if not os.environ.get(\"HF_TOKEN\"):\n",
        "            logging.warning(\"HF_TOKEN not found in environment variables. Model loading may fail.\")\n",
        "\n",
        "        try:\n",
        "            logging.info(f\"Loading tokenizer for {model_id}...\")\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "            logging.info(f\"Loading model {model_id} to {device}...\")\n",
        "            self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                model_id,\n",
        "                device_map=device\n",
        "            )\n",
        "            logging.info(\"Model and tokenizer loaded successfully\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error loading model: {e}\")\n",
        "            raise\n",
        "\n",
        "    def generate_response(self, user_input: str, conversation_history: List[Dict[str, str]] = None) -> str:\n",
        "        try:\n",
        "            # Initialize or update conversation\n",
        "            if conversation_history is None:\n",
        "                messages = [{\"role\": \"user\", \"content\": user_input}]\n",
        "            else:\n",
        "                # Copy conversation history and add new user message\n",
        "                messages = conversation_history.copy()\n",
        "                messages.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "            # Format messages for the model\n",
        "            input_ids = self.tokenizer.apply_chat_template(\n",
        "                messages,\n",
        "                add_generation_prompt=True,\n",
        "                return_tensors=\"pt\"\n",
        "            ).to(self.device)\n",
        "\n",
        "            # Set generation config\n",
        "            generation_config = GenerationConfig(\n",
        "                max_new_tokens=self.max_length,\n",
        "                temperature=0.7,\n",
        "                top_p=0.9,\n",
        "                do_sample=True\n",
        "            )\n",
        "\n",
        "            # Generate response\n",
        "            outputs = self.model.generate(\n",
        "                input_ids,\n",
        "                generation_config=generation_config\n",
        "            )\n",
        "\n",
        "            # Decode the output\n",
        "            decoded_output = self.tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
        "\n",
        "            # Extract assistant's response\n",
        "            start_token = \"<start_of_turn>model\"\n",
        "            end_token = \"<end_of_turn>\"\n",
        "\n",
        "            start_index = decoded_output.find(start_token)\n",
        "            if start_index != -1:\n",
        "                start_index += len(start_token)\n",
        "                end_index = decoded_output.find(end_token, start_index)\n",
        "                if end_index != -1:\n",
        "                    assistant_response = decoded_output[start_index:end_index].strip()\n",
        "                else:\n",
        "                    assistant_response = decoded_output[start_index:].strip()\n",
        "                return assistant_response\n",
        "\n",
        "            # Fallback if couldn't find the tokens\n",
        "            return decoded_output.split(\"<start_of_turn>model\")[-1].split(\"<end_of_turn>\")[0].strip()\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error generating response: {e}\")\n",
        "            return \"I'm sorry, I encountered an error generating a response.\""
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-WMxGE0EjwC"
      },
      "source": [
        "## 5. Audio Processing Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bynggy23EjwC"
      },
      "source": [
        "class AudioProcessor:\n",
        "    \"\"\"Class to handle speech-to-text and text-to-speech operations.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        try:\n",
        "            logging.info(\"Loading STT model...\")\n",
        "            self.stt_model = get_stt_model()\n",
        "            logging.info(\"STT model loaded successfully\")\n",
        "\n",
        "            logging.info(\"Loading TTS model...\")\n",
        "            self.tts_model = get_tts_model()\n",
        "            logging.info(\"TTS model loaded successfully\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error loading audio models: {e}\")\n",
        "            raise\n",
        "\n",
        "    def speech_to_text(self, audio: Tuple[int, np.ndarray]) -> str:\n",
        "        try:\n",
        "            logging.info(\"Converting speech to text...\")\n",
        "            sample_rate, audio_array = audio\n",
        "\n",
        "            # Ensure audio is in the correct format\n",
        "            if len(audio_array.shape) > 1:\n",
        "                audio_array = audio_array.squeeze()\n",
        "\n",
        "            # Convert to text\n",
        "            text = self.stt_model.stt((sample_rate, audio_array))\n",
        "            logging.info(f\"Speech transcribed: '{text}'\")\n",
        "            return text\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error in speech to text conversion: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    def text_to_speech(self, text: str, speaker_id: int = SPEAKER_ID) -> Tuple[int, np.ndarray]:\n",
        "        try:\n",
        "            logging.info(f\"Converting text to speech: '{text}'\")\n",
        "\n",
        "            # Generate speech\n",
        "            sample_rate, audio_array = self.tts_model.tts(text, speaker_id=speaker_id)\n",
        "            logging.info(f\"Text converted to speech. Audio length: {len(audio_array) / sample_rate:.2f}s\")\n",
        "\n",
        "            return sample_rate, audio_array\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error in text to speech conversion: {e}\")\n",
        "            # Return a silent audio segment as fallback\n",
        "            return 16000, np.zeros(16000, dtype=np.float32)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTz1mr76EjwD"
      },
      "source": [
        "## 6. FastAPI Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "mdZ-6W3WEjwD"
      },
      "source": [
        "# Create a simple HTML template (simplified version)\n",
        "html_template = \"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html>\n",
        "<head>\n",
        "    <title>Gemma Voice Assistant</title>\n",
        "    <style>\n",
        "        body { font-family: sans-serif; margin: 0; padding: 20px; }\n",
        "        .chat-container { height: 400px; overflow-y: auto; border: 1px solid #ddd; margin-bottom: 20px; padding: 10px; }\n",
        "        .message { margin-bottom: 10px; padding: 8px; border-radius: 5px; }\n",
        "        .user { background-color: #e3e9f2; margin-left: auto; max-width: 80%; }\n",
        "        .assistant { background-color: #4a6fa5; color: white; max-width: 80%; }\n",
        "        .controls { display: flex; flex-direction: column; gap: 10px; }\n",
        "        .btn { padding: 10px; border: none; border-radius: 5px; background-color: #6c5ce7; color: white; cursor: pointer; }\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "    <h1>Gemma Voice Assistant</h1>\n",
        "    <div class=\"chat-container\" id=\"chat\"></div>\n",
        "    <div class=\"controls\">\n",
        "        <button id=\"mic-btn\" class=\"btn\">Start Recording</button>\n",
        "        <button id=\"clear-btn\" class=\"btn\">Clear Conversation</button>\n",
        "    </div>\n",
        "    <p id=\"status\">Ready</p>\n",
        "    <script>\n",
        "        // WebRTC configuration\n",
        "        const rtcConfiguration = __RTC_CONFIGURATION__;\n",
        "\n",
        "        // Basic functionality - full implementation in the actual HTML file\n",
        "        const chatContainer = document.getElementById('chat');\n",
        "        const micButton = document.getElementById('mic-btn');\n",
        "        const clearButton = document.getElementById('clear-btn');\n",
        "        const statusElement = document.getElementById('status');\n",
        "\n",
        "        let webrtcId = null;\n",
        "        let isRecording = false;\n",
        "        let chatHistory = [];\n",
        "        let conversationState = [];\n",
        "\n",
        "        // Event listeners and WebRTC setup would be implemented here\n",
        "    </script>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "# Define Pydantic models for API\n",
        "class Message(BaseModel):\n",
        "    role: str\n",
        "    content: str\n",
        "\n",
        "class InputData(BaseModel):\n",
        "    webrtc_id: str\n",
        "    chatbot: List[Message]\n",
        "    state: List[Message]\n",
        "    stop: bool = False"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MO9XI5nUEjwD"
      },
      "source": [
        "## 7. Main Application Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_xJj7M7REjwD"
      },
      "source": [
        "# Define response handler\n",
        "def response_handler(\n",
        "    audio: Tuple[int, np.ndarray],\n",
        "    gradio_chatbot: List[Dict] = None,\n",
        "    conversation_state: List[Dict] = None,\n",
        "):\n",
        "    # Initialize if None\n",
        "    gradio_chatbot = gradio_chatbot or []\n",
        "    conversation_state = conversation_state or []\n",
        "\n",
        "    # Get STT model\n",
        "    stt_model = get_stt_model()\n",
        "\n",
        "    # Process audio input (speech to text)\n",
        "    text = stt_model.stt(audio)\n",
        "    logging.info(f\"STT in handler: '{text}'\")\n",
        "\n",
        "    # Add user message to UI\n",
        "    sample_rate, array = audio\n",
        "    gradio_chatbot.append(\n",
        "        {\"role\": \"user\", \"content\": gr.Audio((sample_rate, array.squeeze()))}\n",
        "    )\n",
        "\n",
        "    # First yield to update UI with user's message\n",
        "    yield AdditionalOutputs(gradio_chatbot, conversation_state)\n",
        "\n",
        "    # Add user message to conversation state\n",
        "    conversation_state.append({\"role\": \"user\", \"content\": text})\n",
        "    # Limit conversation history\n",
        "    if len(conversation_state) > MAX_CONTEXT_TURNS * 2:\n",
        "        conversation_state = conversation_state[-MAX_CONTEXT_TURNS * 2:]\n",
        "\n",
        "    # Initialize Gemma model\n",
        "    gemma_model = GemmaModel(device=DEVICE)\n",
        "\n",
        "    # Generate response from Gemma\n",
        "    response_text = gemma_model.generate_response(\n",
        "        text,\n",
        "        conversation_history=conversation_state\n",
        "    )\n",
        "\n",
        "    # Add assistant response to conversation state\n",
        "    assistant_response = {\"role\": \"assistant\", \"content\": response_text}\n",
        "    conversation_state.append(assistant_response)\n",
        "    gradio_chatbot.append(assistant_response)\n",
        "\n",
        "    # Final yield with complete response\n",
        "    yield AdditionalOutputs(gradio_chatbot, conversation_state)\n",
        "\n",
        "# Helper function for API\n",
        "def audio_to_base64(file_path):\n",
        "    audio_format = \"wav\"\n",
        "    with open(file_path, \"rb\") as audio_file:\n",
        "        encoded_audio = base64.b64encode(audio_file.read()).decode(\"utf-8\")\n",
        "    return f\"data:audio/{audio_format};base64,{encoded_audio}\""
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#test"
      ],
      "metadata": {
        "id": "FPsIodFswOAA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def response_handler(\n",
        "    audio: Tuple[int, np.ndarray],\n",
        "    gradio_chatbot: List[Dict] | None = None,\n",
        "    conversation_state: List[Dict] | None = None,\n",
        "):\n",
        "  # Initialize if None\n",
        "  gradio_chatbot = gradio_chatbot or []\n",
        "  conversation_state = conversation_state or []\n",
        "  # Convert speech to text using STT model\n",
        "  text = stt_model.stt(audio)\n",
        "  if not text.strip():\n",
        "      return\n",
        "\n",
        "  # Generate response from Gemma\n",
        "  response_text = gemma_model.generate_response(\n",
        "      text,\n",
        "      conversation_history=conversation_state\n",
        "  )\n",
        "\n",
        "  # Add assistant response to conversation state\n",
        "  assistant_response = {\"role\": \"assistant\", \"content\": response_text}\n",
        "  conversation_state.append(assistant_response)\n",
        "  gradio_chatbot.append(assistant_response)\n",
        "  yield AdditionalOutputs(gradio_chatbot, conversation_state)\n",
        "\n",
        "  for audio_chunk in tts_model.stream_tts_sync(response_text or \"\"):\n",
        "        # Yield the audio chunk\n",
        "        yield audio_chunk\n",
        "\n"
      ],
      "metadata": {
        "id": "adjFTAluwRPm"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9-LeRxJEjwD"
      },
      "source": [
        "## 8. FastAPI App Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "NVvm-Tk0EjwD"
      },
      "source": [
        "def setup_fastapi_app():\n",
        "    # Setup FastAPI app\n",
        "    app = FastAPI()\n",
        "\n",
        "    # Setup FastRTC stream\n",
        "    # Instead of None, provide empty gradio components for additional_inputs\n",
        "    stream = Stream(\n",
        "        ReplyOnStopWords(\n",
        "            response_handler,\n",
        "            stop_words=[\"stop\", \"goodbye\"],\n",
        "            input_sample_rate=16000,\n",
        "        ),\n",
        "        mode=\"send-receive\",\n",
        "        modality=\"audio\",\n",
        "        additional_inputs=[gr.Textbox(visible=False), gr.Textbox(visible=False)],  # Changed to empty Textboxes\n",
        "        additional_outputs=[gr.Textbox(visible=False), gr.Textbox(visible=False)],\n",
        "        additional_outputs_handler=lambda *a: (a[2], a[3]),\n",
        "        concurrency_limit=5 if get_space() else None,\n",
        "        time_limit=90 if get_space() else None,\n",
        "        rtc_configuration=get_twilio_turn_credentials(),\n",
        "    )\n",
        "\n",
        "\n",
        "    # Mount the stream\n",
        "    stream.mount(app)\n",
        "\n",
        "    # Define API endpoints\n",
        "    @app.get(\"/\")\n",
        "    async def root():\n",
        "        rtc_config = get_twilio_turn_credentials() if get_space() else None\n",
        "        html_content = html_template.replace(\"__RTC_CONFIGURATION__\", json.dumps(rtc_config))\n",
        "        return HTMLResponse(content=html_content)\n",
        "\n",
        "    @app.post(\"/input_hook\")\n",
        "    async def input_hook(data: InputData):\n",
        "        body = data.model_dump()\n",
        "        stream.set_input(data.webrtc_id, body[\"chatbot\"], body[\"state\"])\n",
        "\n",
        "    @app.get(\"/outputs\")\n",
        "    async def outputs(webrtc_id: str):\n",
        "        async def output_stream():\n",
        "            async for output in stream.output_stream(webrtc_id):\n",
        "                chatbot = output.args[0]\n",
        "                state = output.args[1]\n",
        "\n",
        "                if not chatbot or not state or not state[-1]:\n",
        "                    continue\n",
        "\n",
        "                data = {\n",
        "                    \"message\": state[-1],\n",
        "                    \"audio\": audio_to_base64(chatbot[-1][\"content\"].value[\"path\"])\n",
        "                    if (chatbot and chatbot[-1][\"role\"] == \"user\" and\n",
        "                        isinstance(chatbot[-1][\"content\"], gr.Audio) and\n",
        "                        hasattr(chatbot[-1][\"content\"], \"value\"))\n",
        "                    else None,\n",
        "                }\n",
        "                yield f\"event: output\\ndata: {json.dumps(data)}\\n\\n\"\n",
        "\n",
        "        return StreamingResponse(output_stream(), media_type=\"text/event-stream\")\n",
        "\n",
        "    return app, stream"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XetlguBGEjwE"
      },
      "source": [
        "## 9. Launch Application"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-WWKaGHpEjwE"
      },
      "source": [
        "def launch_app():\n",
        "    import uvicorn\n",
        "\n",
        "    app, stream = setup_fastapi_app()\n",
        "    mode = os.environ.get(\"MODE\", \"UI\")\n",
        "    stt_model = get_stt_model()\n",
        "    tts_model = get_tts_model()\n",
        "    gemma_model = GemmaModel(device=DEVICE)\n",
        "\n",
        "    if mode == \"UI\":\n",
        "        logging.info(\"Starting FastRTC with Gradio UI\")\n",
        "        stream.ui.launch(server_port=7860)\n",
        "    else:\n",
        "        logging.info(\"Starting FastRTC with FastAPI\")\n",
        "        import nest_asyncio\n",
        "        nest_asyncio.apply()  # This line is added\n",
        "        uvicorn.run(app, host=\"0.0.0.0\", port=7860)\n",
        "\n",
        "# Uncomment the line below to launch the application\n",
        "#launch_app()"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "launch_app()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "ZhaLzlzM7bFZ",
        "outputId": "7296f48c-8326-4ed5-848f-f6396d2d5c52"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "Cannot find empty port in range: 7860-7860. You can specify a different port by setting the GRADIO_SERVER_PORT environment variable or passing the `server_port` parameter to `launch()`.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-b9f852a139e9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlaunch_app\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-f22ac83de59d>\u001b[0m in \u001b[0;36mlaunch_app\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"UI\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting FastRTC with Gradio UI\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mui\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserver_port\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7860\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting FastRTC with FastAPI\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/fastrtc/stream.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"app_kwargs\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"app_kwargs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lifespan\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_lifespan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\u001b[0m in \u001b[0;36mlaunch\u001b[0;34m(self, inline, inbrowser, share, debug, max_threads, auth, auth_message, prevent_thread_lock, show_error, server_name, server_port, height, width, favicon_path, ssl_keyfile, ssl_certfile, ssl_keyfile_password, ssl_verify, quiet, show_api, allowed_paths, blocked_paths, root_path, app_kwargs, state_session_capacity, share_server_address, share_server_protocol, share_server_tls_certificate, auth_dependency, max_file_size, enable_monitoring, strict_cors, node_server_name, node_port, ssr_mode, pwa, _frontend)\u001b[0m\n\u001b[1;32m   2602\u001b[0m                     \u001b[0mlocal_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2603\u001b[0m                     \u001b[0mserver\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2604\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhttp_server\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_server\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2605\u001b[0m                     \u001b[0mapp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2606\u001b[0m                     \u001b[0mserver_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserver_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gradio/http_server.py\u001b[0m in \u001b[0;36mstart_server\u001b[0;34m(app, server_name, server_port, ssl_keyfile, ssl_certfile, ssl_keyfile_password)\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         raise OSError(\n\u001b[0m\u001b[1;32m    157\u001b[0m             \u001b[0;34mf\"Cannot find empty port in range: {min(server_ports)}-{max(server_ports)}. You can specify a different port by setting the GRADIO_SERVER_PORT environment variable or passing the `server_port` parameter to `launch()`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         )\n",
            "\u001b[0;31mOSError\u001b[0m: Cannot find empty port in range: 7860-7860. You can specify a different port by setting the GRADIO_SERVER_PORT environment variable or passing the `server_port` parameter to `launch()`."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PunT5gGREjwE"
      },
      "source": [
        "## 10. Usage Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-zkmW_PEjwE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "4153aefb-2046-4e7a-acbd-d0765fcf09bd"
      },
      "source": [
        "# Example: Create and test a Gemma model instance\n",
        "def test_gemma_model():\n",
        "    # Replace with your actual Hugging Face token\n",
        "    os.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN')\n",
        "\n",
        "    # Use CPU for testing if no GPU available\n",
        "    test_device = \"cuda\"\n",
        "\n",
        "    print(\"Initializing Gemma model...\")\n",
        "    model = GemmaModel(device=test_device)\n",
        "\n",
        "    test_input = \"Hello, how are you today?\"\n",
        "    print(f\"\\nTest input: '{test_input}'\")\n",
        "\n",
        "    print(\"Generating response...\")\n",
        "    response = model.generate_response(test_input)\n",
        "\n",
        "    print(f\"\\nGemma response: '{response}'\")\n",
        "\n",
        "    return response\n",
        "\n",
        "# Uncomment the line below to test the Gemma model\n",
        "test_gemma_model()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing Gemma model...\n",
            "\n",
            "Test input: 'Hello, how are you today?'\n",
            "Generating response...\n",
            "\n",
            "Gemma response: 'Hello there! I’m doing well, thank you for asking! As an AI, I don’t really experience feelings like humans do, but I’m functioning perfectly and ready to help you with whatever you need. \n",
            "\n",
            "How are *you* doing today? 😊 \n",
            "\n",
            "Do you want to chat about something specific, or perhaps need some help with a task?'\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello there! I’m doing well, thank you for asking! As an AI, I don’t really experience feelings like humans do, but I’m functioning perfectly and ready to help you with whatever you need. \\n\\nHow are *you* doing today? 😊 \\n\\nDo you want to chat about something specific, or perhaps need some help with a task?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3qUZiEXEjwE"
      },
      "source": [
        "## How to Use\n",
        "\n",
        "1. Set your Hugging Face API token in the environment variables cell\n",
        "2. Run all cells up to the launch_app() function\n",
        "3. Uncomment and run the launch_app() line to start the application\n",
        "4. Access the interface in your browser at http://localhost:7860\n",
        "\n",
        "You can choose between two interfaces by setting the MODE environment variable:\n",
        "- \"UI\" for Gradio interface\n",
        "- \"WEB\" for FastAPI web interface"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}